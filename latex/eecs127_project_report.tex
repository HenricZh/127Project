% \documentclass{amsart}
\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage[margin=1in]{geometry}
\usepackage{float}

\usepackage{graphicx}
\usepackage{hyperref} 
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}

% \setlength{\parskip}{\baselineskip}
% \setlist{parsep=0.5em, topsep=0.5em, itemsep=0.5em} 
\doublespacing

% Feel free to add any of your own macros, etc here

\title{\vspace{5cm} 
    \textbf{Adversarial Machine Learning} \\
    \vspace{0.25cm}
    \Large{EECS 127 Project}
}  % Feel free to make more descriptive

\author{Neel Kandlikar, Amos You, Henric Zhang}
\date{May 4, 2023}

\begin{document}

\maketitle

\pagebreak

% May want \tableofcontents if you submit a long report with sections
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}

\pagebreak

\section{Introduction}

In recent years, neural networks have proven to be highly effective at various machine learning tasks,
whether it be image classification or speech recognition. Neural networks have achieved state of 
art results for various dataset benchmarks (eg. MNIST, CIFAR, WINE, etc), yet a major issue regarding
the performance of these neural networks remains: susceptibility to adversarial examples. For some 
input $x$ and target label $l$, it is often possible to find a similar input $x'$ such that the 
classifier misclassifies the example as some other label. As more artificial intelligence systems
are being introduced into society and we rely on these deep learning models that are integrated 
ithin these systems, questions regarding the safety and security of these systems are important to
discuss. In security conscious applications such as self driving cars or facial recognition for
mobile devices, ill-intentioned attackers can create adversarial examples that lead to undesired
outcomes, such as steering the car incorrectly or unlocking the phone.

In this paper, we evaluate robustness properties for neural networks. We uncover how we generate 
adversarial examples, what constitutes "robustness" for a neural network, and how robustness 
changes under new conditions for the width and depth of a neural network.

\section{Robustness Properties for Neural Networks}

\subsection{Generating Adversarial Examples}

To generate adversarial examples, we employ the Fast Gradient Signed Method (FGSM), which is an 
efficient algorithm of perturbing inputs. This algorithm takes an input image, and perturbs the
image in the direction of the sign of the gradient with respect to the classifier's loss. In 
pseudocode, this is represented as:

\begin{algorithm}
    \caption{Fast Signed Gradient Method (FGSM)}
\begin{algorithmic}
    \Require $x, l$
    \Require $\epsilon > 0$
    \State $x' \gets x - \epsilon \cdot sign(\nabla loss_{F, l}(x))$
\end{algorithmic}
\end{algorithm}

Unlike other algorithms, Fast Gradient Signed Method is fast due to the fact that the algorithm
only requires the calculation of the gradient, whereas other methods are formulated as a 
minimization problem with convex relaxations and require some sort of line search to find the
"optimal" way to perturb the image. Notice that $\epsilon$ is a hyperparameter here; increasing
$\epsilon$ yields a greater perturbation, lowering $\epsilon$ leads to a smaller perturbation.

\subsection{Baseline}

Our baseline model is a 2 layer neural network, with the hidden layer having 256 nodes, an output
layer having 10 nodes, and ReLU activations between layers. In other words, the dimensionality of 
the layers progresses from 784, to 256, to 10. We see that for adversarial examples, ie. images
perturbed with size $\epsilon$, as given by the Fast Gradient Sign Method, all of these confidence
differences are non-negative. We know that a 0 confidence difference indicates that the model's
confidence in the true class matches class j, and so j must be the true class. For positive
confidence differences, it would indicate that the model is more confident in the true class
than class j, and so for j that is not the true class, we confirm that the neural network is robust.
Had the values been negative, the model would have greater confidence in class j than the true
class, which means that the adversarial example has fooled the neural network. \\

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Class & Confidence in true class - class j \\ \hline
0     & 7.00089168548584                   \\ \hline
1     & 11.432679176330566                 \\ \hline
2     & 1.600733995437622                  \\ \hline
3     & 2.1836435794830322                 \\ \hline
4     & 18.466800689697266                 \\ \hline
5     & 6.655491828918457                  \\ \hline
6     & 14.839714050292969                 \\ \hline
8     & 6.529881477355957                  \\ \hline
9     & 5.659580230712891                  \\ \hline
\end{tabular}
\caption{Confidence Differences for baseline network}
\end{table}

Here, we have $\epsilon = 0.5$. We observe that all the confidence differences are non-negative,
which proves that our model is robust to perturbations of size $0.5$.


\section{Wider and Deeper Networks}

\subsection{Number of Nodes}

Neural networks have a width described by the number of nodes in a particular hidden layer. The 
baseline neural network architecture has only one hidden layer with 256 nodes. We modify this
architecture by increasing the number of nodes from 256 to 1024. In doing so, the formulations
of the optimization problems have remained unchanged for the most part, but the aspect that did 
change was the number of nodes in the hidden layer. 

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
network                       & epsilon               & accuracy           & primal & robust train \\ \hline
\multirow{2}{*}{baseline}     & \multirow{2}{*}{0.05} & accuracy on normal & 0.971  & 0.9532       \\ \cline{3-5} 
                              &                       & accuracy on fgsm   & 0.931  & 0.8948       \\ \hline
\multirow{2}{*}{wide network} & \multirow{2}{*}{0.05} & accuracy on normal & 0.9722 & 0.9544       \\ \cline{3-5} 
                              &                       & accuracy on fgsm   & 0.9365 & 0.9012       \\ \hline
\end{tabular}
\caption{Accuracy for baseline and wide networks}
\end{table}

Upon initial glance, we noticed that the classification accuracy for this model is slightly 
higher than the baseline model, achieving 0.9722 compared to 0.971. This is likely due to the fact 
that this neural network was trained on more nodes, where there is a greater flexibility for the 
neural network to learn the task at hand and be more expressive in determining predictions. After 
perturbing the images via the Fast Gradient Signed Method, the test accuracy has decreased, as 
expected. The unique detail here is that the test accuracy for the robustly trained wide network is
higher than that for the robustly trained baseline network, yielding 0.9012 and 0.8948 respectively. 
In other words, we see that the wider hidden layer led to more robustness, since the lower bound for 
the accuracy of the wide network is higher than the lower bound for the accuracy of the baseline 
network, when we perturb images up to size $\epsilon = 0.05$. Although we only robustly trained on 
images perturbed by this size $\epsilon = 0.05$, it is fair to assume that the robustness quality 
holds as we intensify the perturbations.

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|l|}
\hline
network                       & $\epsilon$               & class & primal      & dual        & robust train \\ \hline
\multirow{9}{*}{baseline}     & \multirow{9}{*}{0.05} & 0     & 20.21192551 & 8.241117477 & 12.31344986  \\ \cline{3-6} 
                              &                       & 1     & 20.19167709 & 11.76900101 & 14.90980721  \\ \cline{3-6} 
                              &                       & 2     & 9.065196037 & 3.281088829 & 7.424779415  \\ \cline{3-6} 
                              &                       & 3     & 9.794664383 & 4.855121136 & 8.22224617   \\ \cline{3-6} 
                              &                       & 4     & 29.35568428 & 17.20711327 & 20.98391914  \\ \cline{3-6} 
                              &                       & 5     & 20.30759811 & 9.340673447 & 13.71678829  \\ \cline{3-6} 
                              &                       & 6     & 35.5247345  & 23.93348312 & 28.84642029  \\ \cline{3-6} 
                              &                       & 8     & 19.39855576 & 9.744720459 & 13.7982502   \\ \cline{3-6} 
                              &                       & 9     & 12.68011379 & 9.952120781 & 12.80408287  \\ \hline
\multirow{9}{*}{wide network} & \multirow{9}{*}{0.05} & 0     & 10.85395241 & 2.003206253 & 17.11705399  \\ \cline{3-6} 
                              &                       & 1     & 13.78579617 & 6.752723217 & 17.49331856  \\ \cline{3-6} 
                              &                       & 2     & 8.221461296 & 1.98862505  & 12.20595074  \\ \cline{3-6} 
                              &                       & 3     & 7.878949165 & 1.389311314 & 7.090931416  \\ \cline{3-6} 
                              &                       & 4     & 18.53999329 & 7.469589233 & 24.26895714  \\ \cline{3-6} 
                              &                       & 5     & 13.85598087 & 4.485163212 & 13.78519058  \\ \cline{3-6} 
                              &                       & 6     & 30.81690979 & 17.43231964 & 29.81845856  \\ \cline{3-6} 
                              &                       & 8     & 12.51810741 & 3.307539463 & 14.2676239   \\ \cline{3-6} 
                              &                       & 9     & 9.359952927 & 1.062012672 & 9.821380615  \\ \hline
\end{tabular}
\caption{Comparison of confidence differences for various networks}
\end{table}

Furthermore, we can again confirm the robustness for the wide neural network through the comparison 
of confidence differences. The adversarial example at hand is a digit 7 perturbed with size $\epsilon
= 0.05$, and we examine the confidence difference between the true class (7) and other target classes.
Both the baseline network and the wide network achieve non-negative confidence differences for all
classes other than 7, which suggests that these neural networks are more confident that the true 
label of the adversarial example is a 7, moreso than when the neural network classifies the example 
as other classes. Meeting this non-negative threshold is a good indicator for robustness, and we see
that both the baseline network and the wide network meet this criterion.

\subsection{Number of Layers}

Beyond changing the width of a neural network, a way to fundamentally change the architecture is by
including additional hidden layers to the neural network. Recall that the baseline architecture had
only one hidden layer with 256 nodes. We built a configuration involving a hidden layer with 1024 
nodes (similar to the previous section), and added in another hidden layer with 256 nodes. This
produced a 3 layer neural network, with layers of dimension 1024, 256, and then 10. 

Configuring the architecture in this manner required a slight modification to the optimization 
problems. The maximum of the Lagrangian (function $g$) will have an additional term of the convex 
relaxation for the ReLU activation after the second layer. Moreover, we need to modify the ReLU 
bounds $\vec{u}$ and $\vec{l}$, in order to compute this function $g$.

Fulfilling this changes, we will be able to robustly train the deep neural network, and assess the
classification accuracy against the adversarial examples. Due to hardware issues, lack of compute, 
and time constraints, we were not able to fully train this deep neural network (8 hour training 
time for wide neural network). However, we can infer some outcomes based on previous results.

In general, including additional hidden layers to a neural network leads to a more accurate
classifier since the additional weights and biases allow for more expressiveness in modeling 
predictions. The expressive nature of a deeper neural network should allow the neural network to
learn from adversarial examples as well, and we expect a robustly trained deep neural network to be
more resilient to adversarial examples, achieving a higher lower bound when assessing classification
accuracy of images perturbed via Fast Gradient Signed Method. Our intuition tells us that if a
wider neural network can achieve some lower bound on the test accuracy when robustly trained, a
deeper neural network that has the same wide layer and additional layers should achieve the same
lower bound or better.

\section{Conclusion}

Although neural networks appear to have blind spots to adversarial examples, it is promising to see
how robustly trained neural networks can combat these concerns and arrive at improved predictions. 
Not to mention, we only observed how neural networks respond to perturbations initiated by the Fast
Gradient Signed Method. Further research can explore how wider and deeper neural networks respond
to more precise adversarial attacks, and observe how robustness is guaranteed in those settings.

\end{document}
