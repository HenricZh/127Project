% \documentclass{amsart}
\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage[margin=1in]{geometry}
\usepackage{float}

\usepackage{graphicx}
\usepackage{hyperref} 
\usepackage{enumitem}
\usepackage{setspace}

% \setlength{\parskip}{\baselineskip}
% \setlist{parsep=0.5em, topsep=0.5em, itemsep=0.5em} 
\doublespacing

% Feel free to add any of your own macros, etc here

\title{\vspace{5cm} 
    \textbf{Adversarial Machine Learning} \\
    \vspace{0.25cm}
    \Large{EECS 127 Project}
}  % Feel free to make more descriptive

\author{Neel Kandlikar, Amos You, Henric Zhang}
\date{May 4, 2023}

\begin{document}

\maketitle

\pagebreak

% May want \tableofcontents if you submit a long report with sections
{
  \hypersetup{linkcolor=black}
  \tableofcontents
}

\pagebreak

\section{Introduction}

In recent years, neural networks have proven to be highly effective at various machine learning tasks,
whether it be image classification or speech recognition. Neural networks have achieved state of 
art results for various dataset benchmarks (eg. MNIST, CIFAR, WINE, etc), yet a major issue regarding
the performance of these neural networks remains: susceptibility to adversarial examples. For some 
input $x$ and target label $l$, it is often possible to find a similar input $x'$ such that the 
classifier misclassifies the example as some other label. As more artificial intelligence systems
are being introduced into society and we rely on these deep learning models that are integrated 
ithin these systems, questions regarding the safety and security of these systems are important to
discuss. In security conscious applications such as self driving cars or facial recognition for
mobile devices, ill-intentioned attackers can create adversarial examples that lead to undesired
outcomes, such as steering the car incorrectly or unlocking the phone.

In this paper, we evaluate robustness properties for neural networks. We uncover how we generate 
adversarial examples, what constitutes "robustness" for a neural network, and how robustness 
changes under new conditions for the width and depth of a neural network.

\section{Robustness Properties for Neural Networks}

\subsection{Generating Adversarial Examples}

To generate adversarial examples, 

\subsection{Baseline}

\section{Wider, Deeper Networks}


\subsection{Number of Nodes}

\subsection{Number of Layers}


\section{Conclusion}




\section{References}



\pagebreak

\printbibliography

\end{document}
